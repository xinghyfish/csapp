# 1 编译器优化的能力和局限性

包括`gcc`在内的现代处理器在编译指令中都会提供优化选项，例如`-O1`或者更高的等级。但是，编译器优化会导致程序体积膨胀和难以使用标准工具debug等问题。

编译器优化需要遵循 *only safe optimzation* 的原则，即优化版本和非优化版本的程序在任何可能遇到的情况中执行逻辑都要保持完全一致。教材提供了的例子揭示了优化时需要注意的重要问题——**内存别名** (*memory alias*) ，也就是多个指针指向同一内存。编译器必须假设不同指针存在 memory alias ，然后进行优化。

由于 memory alias 的存在，导致程序中会出现 *optimization blockers* ，影响程序优化的空间。另一种 optimization blocker 由于函数调用引起。如果调用的函数可能修改全局变量，被称为 *side effect* 。绝大多数编译器都假设出现最坏的情况，保持函数调用的完整性。

教材的Aside也介绍了 *inline substitution/inlining* 的结束，将函数调用转化为函数体代码，以对函数调用进行优化。

---

# 2 描述程序性能

这一节引入了 *cycle per element (CPE)* 这一概念用于描述程序性能，其中 cycle 表示时钟周期， element 表示迭代的元素。

在此基础上，应用 CPE 这一指标衡量了基本的迭代和 *loop unrolling* 操作后的迭代效率，表示讨论的对象不是迭代的轮数 iter ，而是每一个元素 element 。

----

# 3 程序示例

教材给出了一段程序的 baseline 和 gcc 使用`-O1`进行优化后结果，可以看出优化后程序运行效率有了显著的提高。下面针对程序运行的细节进行优化。

----

# 4 消除循环冗余

这一节介绍了一种程序优化的方法—— *code motion* ，这种方法指的是将循环中结果不变但是仍然需要计算多次的值存储到变量中并移动到循环体之前，例如将`vec_length`函数的结果存储在变量`len`中并放在循环之前，这样循环体就可以写成：

```c
for (int i = 0; i < len; i++) {
	// TODO
}
```

由于 code motion 往往涉及函数调用，而上文提到函数调用可能会出现 side effect ，因此程序员者需要手动实现 code motion 。即使编译器支持 inlining ，程序员也要敏锐地察觉到这一点然后就行优化。

----

# 5 减少程序调用

通过对比可以发现，将内部循环的函数调用改成一次函数调用+多次内存访问并不能提高程序运行效率。说明循环内部的部分才是影响程序运行性能的瓶颈。

---

# 6 消除不必要的内存访问

这一节通过分析汇编代码，发现`combine3`中每次计算都需要对`*dest`进行一次读和一次写操作，但是每次读的都是上一轮迭代写入的数据，这样就会耗费多余的步骤在无必要的内存引用上。

参考 code motion 的思想，将这样迭代收尾读写的数据存储在临时的局部变量中，在循环外进行初始化，在迭代时操作局部变量，这样就可以直接调用寄存器而无需进行多余的内存引用。推出循环后，将临时的局部变量写到内存中，这样就只进行了一次内存写操作就完成了数据计算部分的工作。

`combine3`和`combine4`存在 memory aliasing 的问题，显然出现 memory aliasing 的情况不符合我们的期望，但是编译器并不会意识到这个问题，因此编译器只会把`combine3`编译成每一轮迭代进行一次读/写操作的指令而不会进行优化。这需要程序员手动进行优化。

----

# 7 理解现代处理器

为了进一步优化程序，还需要考虑代码运行具体的硬件系统，也就是微处理器架构。由于现代微处理器中存在指令的并行运算，在一些架构中同时可能有超过100条指令运行。

在这一节中，引入了**时延限制** (*latency bound*) 和**吞吐量限制** (*throughput bound*) 来描述程序最大性能上限。当一系列操作必须以严格的顺序执行时就有时延限制；而吞吐量限制则说明了处理器功能单元的计算能力。

## 7.1 全面概述

现代处理器分为**指令控制单元** (*Instruction Controll Unit, ICU*) 和**执行单元** (*Execution Unit, EU*) 两个部分。其中 ICU 用于控制指令的读取， EU 用于控制指令的执行。

当 ICU 的 Fetch Control 将 Address 输入 Instruction Cache 后，会得到对应的指令（二进制数据），之后进行指令的 decode ，输出一系列的 operations/*microoperations* 。

>Operations 是一个更加细化的概念，相比指令，微指令聚焦于更加细微的操作，这些操作的控制逻辑被写到 EU 中。

在 EU 中，每一个微指令都将在对应的**功能单元** (*functional unit*) 执行，其中：
- 内存的读写在 load/store unit 中实现。每个 unit 中都有一个 adder 来进行地址计算。load/store unit 都通过数据缓存 (*data cache*) 来访问内存
- 由于条件跳转指令的存在， EU 的计算结果并不立刻存储到寄存器或内存中，而是等待 branch unit 的结果——如果预测错误，则 EU 的结果将被舍弃；如果预测正确，则进行结果存储
- **代数操作**单元 (*arithmetic operation unit*) 用于进行整型和浮点型数据组合的运算。由于现代微处理器集成度很高，代数操作单元被设计为处理很复杂的运算，否则无法充分利用多计算单元的优势

在 ICU 中，*retirement unit*（有翻译称为回退单元/退役单元）用于跟踪当前正在进行的处理并确保程序按照线性的汇编语义来执行。 Retirement unit 也负责控制 *register file* 的更新。

指令被解码后将被放进一个先进先出的队列。直到 branch predict 的结果出来，指令才被移除队列。如果预测结果正确，则指令将会**回退** (*retire*) ，register file 将被更新；如果分支预测错误，则相关结果都将被舍弃，这样就不会影响程序状态。

为了加速指令间结果的交换，操作结果在 EU 的单元之间通过数据总线可以直接交换，这也是另一种形式的数据转发。

最常用的交换机制是**寄存器重命名** (*register renaming*) 。Decode 之后，待更新寄存器符号`r`和结果标记`t`构成一个条目`(r, t)`存储到表中维护指令更新寄存器的状态。当执行单元完成第一个操作时，生成结果`(v, t)`，其中`v`表示结果的值。因此`v`作为其他使用该寄存器指令的数据源，通过这样的数据传递可以避免 register file 进行寄存器数据的读写。

因此，策略是当解码指令需要寄存器`r`时，同时没有相关的`t`时，就从 register file 进行读写；反之，则从 entry 中读取更新的临时值。

## 7.2 功能单元性能

latency, issue time & capacity 是描述机器指令的重要性质。其中：
- latency：完成操作的总时间
- issue time：两个相同类型独立指令最小时钟周期间隔
- capacity：能过执行该指令的 unit 数量

加法指令、乘法指令等 `issue = 1` 是通过 pipeline 来实现的， pipeline functional unit 则通过一系列 stage 来实现。如果`issue = 1`，则称这种指令是**完全流水线化** (*fully pipelined*) 的——这样的 functional unit 可以在每一个时钟周期开始一条新的指令。

而 divide 操作的则是非流水线化的，一条 devide 指令必须要等待前一条指令完全执行完成后才能开始下一条。因为 divide 操作往往比较复杂，因此上面三个特性的值也是不固定的，值给出了一个范围。

由于集成电路板体积有限，因此 CPU 设计者需要平衡单个 unit 性能和 pipeline 数量的关系。一个通用的原则是为做重要的操作分配最多的资源。

## 7.3 处理器操作的抽象模型

分析程序运行效率可以用数据流图来进行。实质上，程序的状态由内存、寄存器的值来决定，而本节引入的处理器架构涉及到的主要是寄存器状态（处理器中涉及其中的寄存器），因此数据流图关心的是寄存器的读写。

教材提供的数据流图中，蓝色圆角矩形框中包含的是指令，箭头表述数据的流动方向，其中和寄存器相连的箭头表述数据的读写，而和指令相连的表示数据在指令间的转移。

数据流图揭露了数据和寄存器的关系，因此可以进一步将不涉及寄存器读写的指令从图中隐去，仅保留和寄存器读写直接相关的指令（`load`，计算指令），更直观地呈现数据的流动。

数据流图中有一个重要的概念叫做**关键路径** (*critical path*)，这一概念由图论引出，表示对某一个流程执行起关键作用的路径。在这里，关键路径指的是影响程序执行性能瓶颈的路径，例如教材中5.15图中，浮点数乘法指令需要5个时钟周期的延迟，因此浮点数乘法节点构成的路径就是关键路径，而同一层（在同一个迭代之内）的其他指令都可以和乘法指令并行执行。

---

# 8 循环展开

教材上将 $1$ 次循环按照因子 $k$ 展开的思路称为 $k \times 1$ 循环展开。根据教材提供的结果， $k \times 1$ 循环展开当 $k$ 增大后整型数的加法操作 CPE 可以趋近 latency bound 。这样的结果可以归因于将循环的开销操作降低，这样我们目前关注的`OP`操作就可以占主导地位。

通过5.20的关键路径图可以发现，尽管已经进行了循环展开，但是关键路径上仍然有 $n$ 个`OP`指令节点，这就导致 CPE 会卡在 latency bound 这一瓶颈而无法进一步利用好并行性。

---

# 9 并行增强

到这一节，函数已经到达了算数单元的性能瓶颈，但是在处理器架构中存在多个算数单元可以同时进行计算，称为“并行”计算。

在前面的章节中，我们只用一个`acc`变量来存储累积计算结果，因此必须要等到当前计算结束才能开始下一轮计算，这样至少需要等待 $L$ 个时钟周期，因此 $L$ 就称为了 CPE 的瓶颈。为了打破瓶颈，需要利用好并行性，下面介绍这种优化。

## 9.1 乘法累积因子

这一节从纯粹数学的角度出发，通过数列中拆分的思想，把求积运算拆分成两个部分，通过两个累积因子 $PE_n$ 和 $PO_n$ 分别进行累积，之后再把两个因子的乘积作为计算结果。

多累积因子的核心代码如下：

```c
void combine6(vec_ptr v, data *dest)
{
	...
	/* Combine 2 elements at a time */
	for (i = 0; i < limit; i+=2) {
		acc0 = acc0 OP data[i];
		acc1 = acc1 OP data[i+1];
	}
	
	...
	*dest = acc0 OP acc1;
}
```

由于设想的微处理器中至少存在2个算数单元，因此在数据流图中，存在两条平行的关键路径，每条关键路径对应一个算数单元部件。这样，就可以做到两个乘法累积因子对应的部分分别进行计算，打破 latency bound 的瓶颈，挑战 throughput bound 瓶颈。

这样的展开方式称为 $k \times k$ 循环展开。其中第一个 $k$ 表示每次迭代展开 $k$ 个循环，第二个 $k$ 表示循环体内部有 $k$ 个累积因子。

通过分析可以知道，只有当 $k \ge C \cdot L$ 时才能到达 throughput bound ，其中 $C$ 表示算数单元容量，$L$ 表示指令时延。不等式的物理意义指让所有能够执行这类操作的算数单元的流水线保持完全填充的状态。例如浮点数乘法中 $C = 2$，$L = 5$ ，这样 $k \ge 10$ 才能达到瓶颈，这与实验结果相吻合。

## 9.2 计算次序重组

在 $k \times 1$ 循环展开中，并没有改变计算的次序，仅仅只是在一次迭代中展开了 $k$ 次循环：

```c
acc = (acc OP data[i]) OP data[i+1];
```

然而，通过改变计算次序，可以得到：

```c
acc = acc OP (data[i] OP data[i+1]);
```

通过数据流图可以发现，虽然只是对计算的次序进行了简单的更改，但是在这里，每次迭代之间只有一个`%xmm1`寄存器存在上下文的依赖，因此这条寄存器涉及的路径就是关键路径，而执行一次`mult`指令可以过两条数据，因此只需要进行 $n/2$ 次计算即可。因此 CPE 的值也对应除以2。

这样的循环展开被称为 $k \times 1a$ 循环展开。

----

# 10 合并代码优化总结

上文总共介绍了3种循环展开的方式，先针对处理器的结构进行铺垫。核心思想都是将循环的一部分代码进行合并，通过减少循环次数来达到提高程序运行性能的效果。

从教材的表格也可以看出优化前后性能的巨大差异。现代处理器具有相当可观的计算能力，而作为程序员，需要让编写的程序尽可能释放出 CPU 集成的强大算力。

----

# 11 限制因素

本章讨论的影响 CPE 的主要因素为计算量 $N$ （也就是计算总次数）、单条指令在功能单元中的执行周期 $I$ 以及能够执行该操作的单元个数（并行算力） $C$ ，计算公式为

$$
CPE = N \cdot I / C
$$

这个公式提供了程序运行时指令 CPE 的理论下界，但是实际上程序在真实的物理机器上运行时也存在其他影响指令 CPE 的因素。这一章将对其进行探究。

## 11.1 寄存器溢出

这里的 spilling 翻译成“溢出”，和 overflow 并不是一个意思，而是指程序并行度 $P$ 超过了可用寄存器数量时导致部分数据读写需要访问内存。

当 $k \times k$ 循环展开中 $k$ 的值过大时，可能会出现寄存器溢出的问题。现代的 x86-64 处理器包含16个整型寄存器和 16 个 YMM 寄存器来存储浮点数。如果超出这个限制，就会导致出现额外的内存访问，反而会导致 CPE 增大。

## 11.2 分支预测和误判惩罚

由于分支预测可能出现错误的情况，导致 pipeline 需要重新填充，这就是误判惩罚。GCC 编译条件移动指令，在 arch/Practice#4.31 已经知道条件移动指令`cmovXX`并不会产生误判惩罚。

由于分支预测是一项相当复杂的工作，抛开硬件设计，从程序员的角度出发，可以遵循以下原则。

### 不要过于担心分支预测

现代处理器中的分支预测逻辑都非常复杂，能够很好的预测一项常规的和长期的分支，例如循环边界的判断。

在教材提供的边界检查的例子中也可以看出，由于并行计算机制，即使加入了边界检查，对程序性能也只有轻微的影响。

### 编写适合实现条件移动指令的代码

条件移动指令匹配C语言中`x ? y : z`格式的语句。因此，可以将一些非常规的、难以进行预测的条件（例如判断两个数的大小）转换为条件移动指令匹配格式的C语句，可以很好地降低误判惩罚。

----

# 12 理解内存性能

到目前为止，我们的程序处理的都是相对体积较小的数据（小于1000条数据的数组），在这一章中简要讨论数据加载和存储的性能，在下一章中将详细讨论 cache 的功能。

## 12.1 加载性能

由于每个时钟周期至多执行1个加载指令，而有2个计算单元，因此如果一条指令对于每个需要计算的元素至少需要加载 $k$ 条数据，则 CPE 至少为 $k/2$ 。（参考Practice#5.15）

在链表的例子中可以看出，链表下一节点寻址的指令构成了关键路径。如果不从内存读出`next`，就无法进入下一次迭代。函数的 CPE 为4，和机器的 L1 cache 相吻合。

## 12.2 存储性能

单纯的存储操作不会产生依赖，但是存在**写/读依赖** (*write/read dependency*) ，也就是内存读的结果取决于最近的内存写。

教材提供的例子`write_read`函数结合之前提到的 memory alias 现象，说明了不同输入对程序运行性能的影响。由于处理器中存在**数据缓冲** (*data buffer*) 。待存储的数据并不立即写入内存，而是先写到数据缓冲中，以 address-data 条目的形式存储。

读操作执行时需要检查当前读的数据是否被写操作修改，通过查询数据缓冲来实现。如果缓冲中命中相关条目，则说明已经被修改（但没有被写入内存，被挂起），则需要读取数据缓冲；否则读指令可以和写指令同时进行。因此，当输入出现 memory alias 现象时，就会出现写/读依赖，关键路径变为`s_data->load->add`；否则不存在依赖，关键路径为`sub`。

对处理器而言，寄存器的修改可以在 decode 阶段就被捕获，而内存的修改则只能等到加载和修改的地址计算完成，因此，内存系统需要大量的优化使得指令之间可以并行执行。

----

# 13 现实中提高性能的技术

- 顶层设计：选择合适的算法和数据结构
- 基本编程原则：避免出现优化障碍
	- 消除过度的函数调用
	- 消除不必要的内存引用
- 底层优化：利用硬件能力组织代码
	- 循环展开
	- 使用多累积因子、重组等技术来提高指令级并行度
	- 重写条件移动操作，尽可能将条件跳转优化为条件移动

----

# 14 识别并消除性能瓶颈

## 14.1 程序分析(Program Profiling)

Unix系统提供了分析工具`gprof`，具体使用方法参考教材即可。

## 14.2 使用分析器指导优化

这一节结合 Shakespeare 作品文本分析的实例，说明了分析器优化的方案，非常有意思，值的一读。这一节还介绍了一些其他的优化方法，例如哈希函数优化、哈希表优化、排序算法、链表插入方案等。

使用分析器分析程序性能时，首先要将程序整体划分为正交的段落，从编程的角度来说就是分解为不同的函数。之后，运行初始版本的程序并将目光聚焦于木桶的短板，即程序运行最耗时的部分。根据分析器提供的有效信息进行优化方案的选取和实验。